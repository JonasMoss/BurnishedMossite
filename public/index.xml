<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Burnished Mossite</title>
    <link>/</link>
    <description>Recent content on Burnished Mossite</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Handling side effects with the .H function</title>
      <link>/2018/08/h-function/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/h-function/</guid>
      <description>The Problem of Side EffectsWhen I am writing \(\mathtt{R}\) code, I often do stuff in the body of my script that creates undesired side effects.
# n and x constants I wish to use latern = 100x = pi^2/6# lots of code# ...# I suddenly wish to plot somethingn = 1:1000x = 0.1plot(n, pnorm(sqrt(n)*x), type = &amp;quot;l&amp;quot;)Notice that n and x have been rewritten.</description>
    </item>
    
    <item>
      <title>Commentary on Synthese (1977) Part I: Neyman&#39;s Paper</title>
      <link>/2018/08/synthese-neyman-1977/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/synthese-neyman-1977/</guid>
      <description>Synthese is a generalist philosophy journal. It’s usually ranked among the 20 best, usually at the lower end. At least some of its focus is on themes I care about, including decision theory, interpretations of probability, probability paradoxes such as the Sleeping Beauty problem, and, of course, the philosophy of statistics.
And the first issue of the 36th volume of Synthese was devoted to the philosophy of statistics. The occasion was Allan Birnbaum’s passing the year before, and the issue is built around his last submission to the journal.</description>
    </item>
    
    <item>
      <title>A commentary on &#39;Tests of Significance Considered as Evidence&#39; (1942) by Joseph Berkson</title>
      <link>/2018/08/berkson-commentary/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/berkson-commentary/</guid>
      <description>This paper is old, and it shows! He starts of with the following:
There was a time when we did not talk about tests of significance; we simply did them. We tested whether certain quantities we significant in the light of their standard errors, without inquiring as to just what was involved in the procedure, or attempting to generalize it.
Sounds like the golden age of statistics! But the twilight of that age had long passed, for when he wrote this paper, statistics “consists almost entirely of tests of significance”.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>I&amp;rsquo;m Jonas Moss, a statistics PhD student at the University of Oslo. I have three kids, two cats, and one wife. I&amp;rsquo;m interested in R programming, the replication crisis in psychology, theoretical statistics, effective altruism, and rationalism à la Lesswrong and Slate Star Codex.
This blog is made in RStudio with blogdown, using Hugo and the Tranquilpeak theme. The .Rmd source files for all posts are available on GitHub.
The mineral to the right is a tapiolite, which is related to the mineral mossite this blog is named after.</description>
    </item>
    
    <item>
      <title>Optional stopping, streaks, and champagne</title>
      <link>/2018/04/optional-stopping-streaks/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/optional-stopping-streaks/</guid>
      <description>At the Psychological Methods Discussion group, Ben Ambridge asked the following question:
Hi everyone - I was wondering (don’t worry, I haven’t actually done this!) what would be wrong statistically speaking with an approach where you run a frequentist t-test (or whatever) after adding each participant and stop testing participants when the p value has remained below 0.05 (or 0.001 or whatever) for - say - each of the last 20 participants.</description>
    </item>
    
    <item>
      <title></title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>